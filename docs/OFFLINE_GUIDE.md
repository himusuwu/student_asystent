# ğŸš€ Student Assistant - Tryb Offline z Qwen

## âœ… System jest skonfigurowany do pracy offline!

### ğŸ“‹ Co dziaÅ‚a offline:

1. **Frontend** - PeÅ‚ny interfejs uÅ¼ytkownika
2. **Backend** - Lokalny serwer Node.js (port 3001)
3. **Whisper** - Transkrypcja audio (modele local)
4. **Qwen 2.5:14b** - Generowanie tytuÅ‚Ã³w i notatek (przez Ollama)
5. **PDF** - Ekstrakcja tekstu (dziaÅ‚a w przeglÄ…darce)
6. **PowerPoint** - Ekstrakcja tekstu (przez backend)

---

## ğŸ¯ Szybki start (3 komendy)

```bash
# Terminal 1: Ollama (Qwen)
ollama serve

# Terminal 2: Backend
cd server && npm start

# Terminal 3: Frontend
npm run dev
```

**OtwÃ³rz:** http://localhost:8000

---

## ğŸ§ª Test systemu

OtwÃ³rz w przeglÄ…darce:
```
http://localhost:8000/test-offline.html
```

Ten test automatycznie sprawdzi:
- âœ… Backend dziaÅ‚a
- âœ… Ollama dziaÅ‚a
- âœ… Qwen 2.5:14b jest dostÄ™pny
- âœ… Generowanie tytuÅ‚Ã³w dziaÅ‚a

---

## ğŸ› RozwiÄ…zywanie problemÃ³w

### Problem: "Przycisk Nowy wykÅ‚ad nie dziaÅ‚a"

**RozwiÄ…zanie:**
1. OtwÃ³rz konsolÄ™ przeglÄ…darki (F12)
2. SprawdÅº czy sÄ… bÅ‚Ä™dy
3. OdÅ›wieÅ¼ stronÄ™ (Cmd+R / Ctrl+R)
4. SprawdÅº czy wszystkie serwery dziaÅ‚ajÄ…:

```bash
# SprawdÅº frontend
curl http://localhost:8000

# SprawdÅº backend
curl http://localhost:3001/health

# SprawdÅº Ollama
curl http://localhost:11434/api/tags
```

### Problem: "Ollama nie dziaÅ‚a"

```bash
# Uruchom Ollama
ollama serve

# SprawdÅº czy Qwen jest zainstalowany
ollama list

# JeÅ›li nie ma, zainstaluj
ollama pull qwen2.5:14b
```

### Problem: "Backend nie odpowiada"

```bash
# SprawdÅº czy port jest zajÄ™ty
lsof -i :3001

# JeÅ›li tak, zabij proces
kill -9 <PID>

# Uruchom ponownie
cd server && npm start
```

### Problem: "Generowanie tytuÅ‚u trwa wiecznie"

**Przyczyny:**
- Qwen 2.5:14b potrzebuje ~15-30 sekund (to normalne!)
- Model jest duÅ¼y (14 miliardÃ³w parametrÃ³w)
- Pierwsza generacja moÅ¼e trwaÄ‡ dÅ‚uÅ¼ej (Å‚adowanie modelu)

**RozwiÄ…zanie:**
- Poczekaj cierpliwie 30-60 sekund
- Lub uÅ¼yj mniejszego modelu:

```bash
# Zainstaluj szybszy model
ollama pull phi3.5:3.8b

# W server/server.js zmieÅ„:
const model = 'phi3.5:3.8b'  // Zamiast qwen2.5:14b
```

---

## ğŸ“Š PorÃ³wnanie modeli

| Model | Rozmiar | SzybkoÅ›Ä‡ | JakoÅ›Ä‡ |
|-------|---------|----------|--------|
| phi3.5:3.8b | 2.3 GB | âš¡âš¡âš¡ Bardzo szybki (5-10s) | â­â­â­ Dobra |
| qwen2.5:14b | 9.0 GB | âš¡âš¡ Åšredni (15-30s) | â­â­â­â­â­ DoskonaÅ‚a |
| llama3.1:8b | 4.7 GB | âš¡âš¡ Åšredni (10-20s) | â­â­â­â­ Bardzo dobra |

**Rekomendacja:** 
- **Qwen 2.5:14b** - Najlepsza jakoÅ›Ä‡ (uÅ¼ywaj tego!)
- **phi3.5:3.8b** - JeÅ›li potrzebujesz szybkoÅ›ci
- **llama3.1:8b** - Dobry kompromis

---

## ğŸ“ PrzykÅ‚adowy workflow

### Scenariusz 1: Nagranie wykÅ‚adu

```
1. Nowy wykÅ‚ad â†’ Wybierz przedmiot
2. ğŸ¤ Nagranie audio â†’ Rozpocznij nagrywanie
3. Nagraj wykÅ‚ad (lub wgraj plik audio)
4. Zatrzymaj â†’ PotwierdÅº transkrypcjÄ™
5. â³ Poczekaj ~2 minuty (transkrypcja)
6. â³ Poczekaj ~30 sekund (AI generuje tytuÅ‚)
7. Edytuj tytuÅ‚ jeÅ›li potrzebujesz
8. Zapisz wykÅ‚ad âœ…
```

### Scenariusz 2: PDF/PowerPoint

```
1. Nowy wykÅ‚ad â†’ Wybierz przedmiot
2. ğŸ“„ Dokument (PDF/PPT)
3. Wgraj PDF lub PPTX
4. â³ Poczekaj ~10 sekund (ekstrakcja)
5. â³ Poczekaj ~30 sekund (AI generuje tytuÅ‚)
6. Zapisz wykÅ‚ad âœ…
```

### Scenariusz 3: Generowanie materiaÅ‚Ã³w

```
1. OtwÃ³rz wykÅ‚ad
2. Kliknij zakÅ‚adkÄ™:
   - ğŸ“ Notatki â†’ Generuj z AI
   - ğŸ´ Fiszki â†’ Generuj z AI
   - ğŸ“ Quiz â†’ Generuj z AI
3. â³ Poczekaj ~30-60 sekund (Qwen generuje)
4. Gotowe! Ucz siÄ™! ğŸ‰
```

---

## âš¡ Optymalizacja

### Przyspieszenie Qwen

1. **UÅ¼ywaj GPU** (jeÅ›li masz):
   ```bash
   # Ollama automatycznie wykrywa GPU
   # Na M4 Pro uÅ¼ywa Metal (juÅ¼ skonfigurowane!)
   ```

2. **ZwiÄ™ksz limity pamiÄ™ci**:
   ```bash
   # W ~/.ollama/config.json
   {
     "gpu_layers": 99,
     "context_size": 4096
   }
   ```

3. **Pre-load model**:
   ```bash
   # ZaÅ‚aduj Qwen przed uÅ¼yciem aplikacji
   ollama run qwen2.5:14b "test"
   ```

---

## ğŸ“š Dodatkowe informacje

### Gdzie sÄ… modele?

```bash
# Ollama models
~/.ollama/models/

# Whisper models
./public/models/Xenova/
```

### Ile miejsca zajmuje?

- **Qwen 2.5:14b**: ~9 GB
- **Whisper base**: ~142 MB
- **Whisper small**: ~466 MB
- **Razem**: ~10 GB

### Czy mogÄ™ uÅ¼ywaÄ‡ bez internetu?

âœ… **TAK!** Po zainstalowaniu wszystkich modeli:
- Transkrypcja: 100% offline
- Generowanie tytuÅ‚Ã³w: 100% offline (Qwen local)
- Ekstrakcja PDF: 100% offline (PDF.js w przeglÄ…darce)
- Ekstrakcja PPTX: wymaga backendu (local, offline)

---

## ğŸ‰ Podsumowanie

TwÃ³j system jest **w peÅ‚ni funkcjonalny offline**:

âœ… Backend dziaÅ‚a (Node.js)  
âœ… Ollama dziaÅ‚a  
âœ… Qwen 2.5:14b jest dostÄ™pny  
âœ… Whisper jest gotowy  
âœ… Frontend dziaÅ‚a  

**MoÅ¼esz teraz uÅ¼ywaÄ‡ Student Assistant bez internetu!**

---

## ğŸ“ Pomoc

JeÅ›li coÅ› nie dziaÅ‚a:
1. SprawdÅº http://localhost:8000/test-offline.html
2. Zobacz logi w terminalu
3. OtwÃ³rz issue na GitHub
4. SprawdÅº dokumentacjÄ™: docs/TROUBLESHOOTING.md

**Happy offline learning!** ğŸš€
