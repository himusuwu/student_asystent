# âš ï¸ Problemy z transkrypcjÄ… audio

## ğŸš¨ Problem 1: "Unsupported model type: whisper"

### ğŸ” Co siÄ™ staÅ‚o?

DostaÅ‚eÅ› bÅ‚Ä…d:
```
Unsupported model type: whisper
```

### ğŸ’¡ Przyczyna

**transformers.js** (biblioteka do uruchamiania AI w przeglÄ…darce) wymaga modeli w formacie **ONNX** (`.onnx` pliki).

TwÃ³j model `whisper-tiny` jest w formacie **PyTorch** (`.bin`) zamiast ONNX!

### Sprawdzenie:

```bash
# Model BEZ ONNX (âŒ nie dziaÅ‚a):
ls public/models/Xenova/whisper-tiny/
# Wynik: pytorch_model.bin, tf_model.h5 (brak folderu onnx/)

# Model Z ONNX (âœ… dziaÅ‚a):
ls public/models/Xenova/whisper-base/onnx/
# Wynik: encoder_model.onnx, decoder_model.onnx, etc.
```

---

## ğŸš¨ Problem 2: "Attempting to extract features for audio longer than 30 seconds"

### ğŸ” Co siÄ™ staÅ‚o?

Dostajesz ostrzeÅ¼enie:
```
Attempting to extract features for audio longer than 30 seconds. 
If using a pipeline to extract transcript from a long audio clip, 
remember to specify chunk_length_s and/or stride_length_s.
```

### ğŸ’¡ Przyczyna

Whisper domyÅ›lnie przetwarza audio w 30-sekundowych fragmentach. Dla dÅ‚ugich nagraÅ„ (>30s) musisz jawnie ustawiÄ‡ parametry `chunk_length_s` i `stride_length_s`.

### âœ… ROZWIÄ„ZANIE

**Backend juÅ¼ zostaÅ‚ zoptymalizowany!** SprawdÅº plik `server/server.js`:

```javascript
const result = await transcriber(pcm, {
  chunk_length_s: 30,     // Optymalne dla M4 Pro âœ…
  stride_length_s: 5,     // Overlap miÄ™dzy chunkami âœ…
  // ... inne parametry
})
```

### ğŸ“Š Oczekiwane czasy na M4 Pro

| DÅ‚ugoÅ›Ä‡ audio | Czas transkrypcji | Ratio |
|---------------|-------------------|-------|
| 1 minuta      | ~5-8 sekund       | ~8-10x |
| 10 minut      | ~50-90 sekund     | ~7-12x |
| 60 minut      | ~6-10 minut       | ~6-10x |

> **Zobacz**: `PERFORMANCE_M4.md` dla szczegÃ³Å‚Ã³w optymalizacji

---

## âœ… ROZWIÄ„ZANIA dla "Unsupported model"

### **Opcja 1: UÅ¼ywaj modelu `base`** (najszybsze)

Masz juÅ¼ `whisper-base` w formacie ONNX! Po prostu:

1. **OtwÃ³rz aplikacjÄ™ â†’ Settings**
2. **ZmieÅ„ "Model Whisper" na: `base`**
3. **Gotowe!** âœ…

> **Uwaga**: DomyÅ›lnie aplikacja teraz uÅ¼ywa `base` zamiast `tiny`. MoÅ¼esz zostawiÄ‡ to ustawienie.

---

### **Opcja 2: Pobierz `whisper-tiny` w ONNX**

JeÅ›li koniecznie chcesz uÅ¼ywaÄ‡ `tiny`:

#### Automatycznie (zalecane):
```bash
./download-whisper-tiny-onnx.sh
```

#### Manualnie:
```bash
cd public/models/Xenova
rm -rf whisper-tiny  # UsuÅ„ starÄ… wersjÄ™

# Klonuj z sparse checkout (tylko ONNX)
git clone --depth 1 --filter=blob:none --sparse https://huggingface.co/Xenova/whisper-tiny
cd whisper-tiny
git sparse-checkout set onnx
git lfs pull
```

---

### **Opcja 3: UÅ¼ywaj backendu** (najlepsze dla duÅ¼ych nagraÅ„!)

Backend **nie potrzebuje ONNX** - dziaÅ‚a z dowolnym formatem modelu:

```bash
# Terminal 1: Uruchom backend
cd server
npm start

# Terminal 2: Uruchom aplikacjÄ™
npm run dev
```

W aplikacji: **Settings â†’ Transkrypcja: Auto (backend first)**

Backend automatycznie:
- âœ… Pobiera model w poprawnym formacie
- âœ… Jest 5-10x szybszy od przeglÄ…darki
- âœ… Nie blokuje UI

---

## ğŸ“Š PorÃ³wnanie opcji

| Opcja | SzybkoÅ›Ä‡ | ZÅ‚oÅ¼onoÅ›Ä‡ | Dla duÅ¼ych nagraÅ„ (90min) |
|-------|----------|-----------|---------------------------|
| **Base w przeglÄ…darce** | ğŸŒ Åšrednia | âœ… Bardzo prosta | âŒ 90-120 min |
| **Tiny w przeglÄ…darce** | ğŸŒ Szybsza | ğŸ”§ Wymaga pobrania ONNX | âŒ 60-90 min |
| **Backend (zalecane!)** | ğŸš€ Najszybsza | âœ… Prosta | âœ… 9-18 min |

---

## ğŸ¯ Zalecenie

Dla twoich **90-minutowych nagraÅ„ (600MB)**:

### âš¡ **UÅ¼yj backendu!**

1. `cd server && npm start`
2. Aplikacja automatycznie wykryje backend
3. Transkrypcja: ~10-20 minut zamiast >60 minut!

PrzeglÄ…darkowe modele sÄ… OK dla **krÃ³tkich nagraÅ„ (<10 min)**, ale dla dÅ‚ugich wykÅ‚adÃ³w backend jest **niezbÄ™dny**.

---

## ğŸ› Dalsze problemy?

SprawdÅº logi:
- OtwÃ³rz konsolÄ™ (F12)
- Szukaj `[AI]` w logach
- SprawdÅº czy widzisz: `Local model detected!` lub `Using remote`

Lub uruchom diagnostykÄ™:
```bash
./check-backend.sh
```
